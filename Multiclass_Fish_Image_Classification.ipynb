{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "HhfV-JJviCcP",
        "nA9Y7ga8ng1Z",
        "dauF4eBmngu3",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "KSlN3yHqYklG",
        "EM7whBJCYoAo",
        "4Of9eVA-YrdM",
        "bamQiAODYuh1",
        "OH-pJp9IphqM",
        "n3dbpmDWp1ck",
        "g-ATYxFrGrvw",
        "8yEUt7NnHlrM",
        "4_0_7-oCpUZd",
        "bn_IUdTipZyH",
        "OB4l2ZhMeS1U",
        "Y3lYRqnEy0qL",
        "Zvg2p4m_zAMa",
        "dJ2tPlVmpsJ0",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "dirZgsisYB1N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "EyNgTHvd2WFk",
        "aa99ad13",
        "gCX9965dhzqZ"
      ],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ksh1t1zsharma/Netflix-Movies-and-TV-Shows-Clustering/blob/main/Multiclass_Fish_Image_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multiclass Fish Image Classification**"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This project, titled Multiclass Fish Image Classification, focuses on developing a robust deep learning pipeline to accurately classify images of fish into multiple species. The goal is to compare different model architectures, identify the most effective one, and prepare the solution for deployment through a Streamlit-based web application.\n",
        "- The dataset comprises images of various fish species organized into training, validation, and testing folders. Each subfolder within these sets corresponds to a distinct fish class. The data was preprocessed using TensorFlow’s ImageDataGenerator, applying rescaling to normalize pixel values to the range and employing augmentation techniques such as random rotations, shifts, shears, zooms, and horizontal flips. This augmentation step aimed to improve model generalization and handle potential overfitting.\n",
        "- An extensive Exploratory Data Analysis (EDA) was conducted which included:\n",
        " - Count plots & pie charts to understand the distribution of classes.\n",
        " - Sample image display to visually validate labeling correctness.\n",
        " - Dimension and color channel checks to ensure data consistency.\n",
        " - Brightness and average color variation analysis to explore visual differences between species.\n",
        "- Statistical hypothesis testing (Chi-square, ANOVA) to identify patterns and imbalances.\n",
        "The Model Training phase followed the assignment requirement to:\n",
        " - Train a CNN from scratch – a custom convolutional neural network was designed with multiple Conv2D, MaxPooling, and Dense layers. While simpler than pre-trained architectures, it set a solid baseline.\n",
        " - Experiment with five pre-trained models using Transfer Learning:\n",
        "   1. VGG16\n",
        "   2. ResNet50\n",
        "   3. MobileNet\n",
        "   4. InceptionV3\n",
        "   5. EfficientNetB0\n",
        "   \n",
        "- For each pre-trained model, ImageNet weights were loaded, the convolutional base was frozen initially, and new dense layers were added for classification into the target species. Models were compiled with the Adam optimizer, categorical cross-entropy loss, and accuracy as the evaluation metric.\n",
        "- To further optimize performance, we conducted hyperparameter tuning on each architecture by experimenting with different learning rates, dense layer units, and dropout rates. In some cases, fine-tuning was performed by unfreezing selected higher convolutional layers and re-training with a lower learning rate.\n",
        "- Model evaluation was based on:\n",
        " - Accuracy, Precision, Recall, F1-score\n",
        "Confusion Matrices to visualize class-wise predictions\n",
        " - Training history plots (accuracy and loss) for monitoring learning behavior.\n",
        "- Results demonstrated that while the custom CNN performed reasonably well, transfer learning models significantly outperformed it. Among them, MobileNet (example — replace with your highest) achieved the highest validation and test accuracy, making it the chosen model for deployment.\n",
        "- Following evaluation, the best-performing model was saved in .keras format and integrated into a Streamlit web application. The app allows users to:\n",
        " - Upload a fish image (JPG/PNG)\n",
        " - Automatically preprocess the image\n",
        " - Predict the species and display it along with a confidence score\n",
        " - Show a real-time classification result through an intuitive interface\n",
        "\n",
        "- Business Impact & Use Cases:\n",
        " - This classification system can help marine biologists, fishermen, and seafood distributors quickly identify fish species, reducing human error and improving operational efficiency. It can also support biodiversity studies and assist in monitoring fish populations.\n",
        " - Key Learnings & Skills Acquired:\n",
        "  - Data preprocessing and augmentation for image models\n",
        "  - Building CNNs from scratch\n",
        "  - Applying Transfer Learning and fine-tuning\n",
        "  - Model evaluation with detailed statistical and visual tools\n",
        "  - Deployment using Streamlit and ngrok\n",
        "  - Hands-on experience with TensorFlow/Keras and Python-based ML workflows\n",
        "- Conclusion:\n",
        " - The project successfully delivered a full machine learning solution — from initial data exploration to deployed application. Systematic experimentation and evaluation allowed selection of an optimal architecture, demonstrating the effectiveness of transfer learning for domain-specific image classification tasks. The pipeline is scalable, allowing more classes or updated datasets to be incorporated with minimal changes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/kshitiz562"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The identification of fish species from images is a critical challenge in fisheries management and biodiversity conservation. Manual classification is often slow and requires expert knowledge, highlighting the need for automated solutions.\n",
        "\n",
        "- This project aims to automate fish species classification using deep learning and transfer learning techniques. We will develop and optimize various convolutional neural network (CNN) architectures, including VGG16, ResNet50, MobileNet, InceptionV3, and EfficientNetB0, on a labeled fish image dataset.\n",
        "\n",
        "- Key challenges such as class imbalance and variable image quality will be addressed, and a robust training pipeline with data augmentation will be established. Furthermore, the final model will be integrated into a Streamlit web application for real-time predictions and confidence scores.\n",
        "\n",
        "- By providing an efficient and user-friendly classification system, this project will enhance the automation of fish species identification and support large-scale monitoring in environmental and commercial applications."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "\n",
        "!pip install numpy pandas matplotlib seaborn opencv-python pillow tensorflow scikit-learn optuna shap streamlit opencv-python pyngrok ngrok"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xmAqrzAVny_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# General Utilities\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Deep Learning - TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Pre-trained Model Architectures\n",
        "from tensorflow.keras.applications import (\n",
        "    VGG16,\n",
        "    ResNet50,\n",
        "    MobileNet,\n",
        "    InceptionV3,\n",
        "    EfficientNetB0\n",
        ")\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input as mobilenet_preprocess\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Model Serialization\n",
        "import pickle\n",
        "\n",
        "# Streamlit for Deployment\n",
        "import streamlit as st\n",
        "\n",
        "# Utilities\n",
        "from PIL import Image\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "# Load the zipfile\n",
        "import os\n",
        "import zipfile\n",
        "zip_file_path = '/content/drive/MyDrive/Dataset.zip'\n",
        "extract_path='/content/fish_images'\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "    print(\"Dataset has been extracted successfully.\")\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Define the main dataset directory where you've extracted your images\n",
        "dataset_path = '/content/fish_images/images.cv_jzk6llhf18tm3k0kyttxz/data'\n",
        "\n",
        "# Define subdirectories for train, validation, and test datasets\n",
        "train_dir = os.path.join(dataset_path, 'train')\n",
        "val_dir = os.path.join(dataset_path, 'val')\n",
        "test_dir = os.path.join(dataset_path, 'test')\n",
        "\n",
        "# Set image size and batch size suitable for most transfer learning architectures\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "# Create ImageDataGenerator instances with data augmentation for training and normalization for all\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    zoom_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create dataset generators\n",
        "train_dataset = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_dataset = val_test_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_dataset = val_test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Check class labels mapping\n",
        "print(\"Class indices:\", train_dataset.class_indices)"
      ],
      "metadata": {
        "id": "ogVjuB2ayE7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Images Dataset"
      ],
      "metadata": {
        "id": "M_jpgAty1c98"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d1418a7"
      },
      "source": [
        "# Define the base directory where the fish images are stored\n",
        "dataset_path = '/content/fish_images/images.cv_jzk6llhf18tm3k0kyttxz/data'\n",
        "\n",
        "# Define subdirectories for train, validation, and test datasets\n",
        "train_dir = os.path.join(dataset_path, 'train')\n",
        "val_dir = os.path.join(dataset_path, 'val')\n",
        "test_dir = os.path.join(dataset_path, 'test')\n",
        "\n",
        "# Create empty lists to store the file paths and labels for each dataset\n",
        "train_filepaths = []\n",
        "train_labels = []\n",
        "val_filepaths = []\n",
        "val_labels = []\n",
        "test_filepaths = []\n",
        "test_labels = []\n",
        "\n",
        "# Iterate through each class directory in the training set\n",
        "for class_name in os.listdir(train_dir):\n",
        "    class_dir = os.path.join(train_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for image_name in os.listdir(class_dir):\n",
        "            image_path = os.path.join(class_dir, image_name)\n",
        "            train_filepaths.append(image_path)\n",
        "            train_labels.append(class_name)\n",
        "\n",
        "# Iterate through each class directory in the validation set\n",
        "for class_name in os.listdir(val_dir):\n",
        "    class_dir = os.path.join(val_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for image_name in os.listdir(class_dir):\n",
        "            image_path = os.path.join(class_dir, image_name)\n",
        "            val_filepaths.append(image_path)\n",
        "            val_labels.append(class_name)\n",
        "\n",
        "# Iterate through each class directory in the test set\n",
        "for class_name in os.listdir(test_dir):\n",
        "    class_dir = os.path.join(test_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for image_name in os.listdir(class_dir):\n",
        "            image_path = os.path.join(class_dir, image_name)\n",
        "            test_filepaths.append(image_path)\n",
        "            test_labels.append(class_name)\n",
        "\n",
        "print(f\"Number of training images found: {len(train_filepaths)}\")\n",
        "print(f\"Number of validation images found: {len(val_filepaths)}\")\n",
        "print(f\"Number of test images found: {len(test_filepaths)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8cc9c5e"
      },
      "source": [
        "# Create DataFrames\n",
        "train_df = pd.DataFrame({'filepath': train_filepaths, 'label': train_labels})\n",
        "val_df = pd.DataFrame({'filepath': val_filepaths, 'label': val_labels})\n",
        "test_df = pd.DataFrame({'filepath': test_filepaths, 'label': test_labels})\n",
        "\n",
        "# Display the head of each DataFrame\n",
        "print(\"Train DataFrame Head:\")\n",
        "display(train_df.head())\n",
        "\n",
        "print(\"\\nValidation DataFrame Head:\")\n",
        "display(val_df.head())\n",
        "\n",
        "print(\"\\nTest DataFrame Head:\")\n",
        "display(test_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "063945d5"
      },
      "source": [
        "train_df.to_csv('/content/drive/MyDrive/my_data/train.csv', index=False)\n",
        "val_df.to_csv('/content/drive/MyDrive/my_data/val.csv', index=False)\n",
        "test_df.to_csv('/content/drive/MyDrive/my_data/test.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "train=pd.read_csv('/content/drive/MyDrive/my_data/train.csv')\n",
        "val=pd.read_csv('/content/drive/MyDrive/my_data/val.csv')\n",
        "test=pd.read_csv('/content/drive/MyDrive/my_data/test.csv')\n",
        "print(train.head())\n",
        "print(val.head())\n",
        "print(test.head())"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Train Dataset Rows and Columns:\", train.shape)\n",
        "print(\"Validation Dataset Rows and Columns:\", val.shape)\n",
        "print(\"Test Dataset Rows and Columns:\", test.shape)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "print(\"Train Dataset Information:\")\n",
        "print(train.info())\n",
        "\n",
        "print(\"\\nValidation Dataset Information:\")\n",
        "print(val.info())\n",
        "\n",
        "print(\"\\nTest Dataset Information:\")\n",
        "print(test.info())"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"Train Dataset Duplicate Values:\")\n",
        "print(train.duplicated().sum())\n",
        "\n",
        "print(\"\\nValidation Dataset Duplicate Values:\")\n",
        "print(val.duplicated().sum())\n",
        "\n",
        "print(\"\\nTest Dataset Duplicate Values:\")\n",
        "print(test.duplicated().sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"Train Dataset Missing Values:\")\n",
        "print(train.isnull().sum())\n",
        "\n",
        "print(\"\\nValidation Dataset Missing Values:\")\n",
        "print(val.isnull().sum())\n",
        "\n",
        "print(\"\\nTest Dataset Missing Values:\")\n",
        "print(test.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a heatmap to visualize missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(train.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Train Missing Values Heatmap')\n",
        "plt.show()\n",
        "sns.heatmap(val.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Valid Missing Values Heatmap')\n",
        "plt.show()\n",
        "sns.heatmap(test.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Test Missing Values Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Prior to initiating the data analysis and model development processes, the foundational understanding of the dataset was derived from its source and accompanying description. This dataset is recognized as a compilation of images designated for multiclass fish classification. It is anticipated that the images will depict a variety of distinct fish species, with each image linked to an appropriate class label.\n",
        "\n",
        "- It is expected that the dataset is structured into directories, with each directory potentially corresponding to a specific fish class. The images themselves are presumed to be standard photographic representations of fish. However, specific data attributes such as the total number of classes, the distribution of images across classes, image dimensions, and any potential data quality concerns remained unknown prior to the exploratory data analysis phase.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"Train Dataset Columns:\")\n",
        "print(train.columns)\n",
        "\n",
        "print(\"\\nValidation Dataset Columns:\")\n",
        "print(val.columns)\n",
        "\n",
        "print(\"\\nTest Dataset Columns:\")\n",
        "print(test.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "print(\"Train Dataset Describe:\")\n",
        "print(train.describe())\n",
        "\n",
        "print(\"\\nValidation Dataset Describe:\")\n",
        "print(val.describe())\n",
        "\n",
        "print(\"\\nTest Dataset Describe:\")\n",
        "print(test.describe())"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Upon converting the dataset into DataFrames labeled as train_df, val_df, and test_df, each dataset comprises the following variables (columns):\n",
        "\n",
        "**Variable Name** | **Data Type** | **Description**  \n",
        "--- | --- | ---  \n",
        "filepath | string | The complete file path to the image stored on disk. This is utilized by the model loading pipeline (ImageDataGenerator) to read and preprocess the corresponding image.  \n",
        "label | string | The target class name (fish species) associated with the image. This label is derived from the folder name in which the image resides and serves as the dependent variable for the classification task.\n",
        "\n",
        "- In addition, several temporary variables created during the analysis, which are not included in the final training dataset, include:\n",
        "\n",
        "**Variable Name** | **Data Type** | **Description**  \n",
        "--- | --- | ---  \n",
        "filepath_length | int | The length (number of characters) of the image's file path, employed solely for exploratory purposes to identify any unusually long paths or storage inconsistencies.  \n",
        "avg_color | list[float] | The average RGB values calculated for the image, utilized in Exploratory Data Analysis (EDA) to examine potential color distribution differences across classes.  \n",
        "brightness | float | The average grayscale intensity value of the image, intended for analyzing brightness distribution.\n",
        "\n",
        "- The label field is categorical, containing values that correspond to the names of fish species. The filepath field is unique for each image and facilitates the location and loading of images for training or testing purposes. During the preprocessing phase for modeling, the label variable is one-hot encoded into numerical vectors, while the filepath variable is employed to provide images to both the Convolutional Neural Network (CNN) and transfer learning models."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(\"Unique Values in Train Dataset:\")\n",
        "print(train.nunique())\n",
        "\n",
        "print(\"\\nUnique Values in Validation Dataset:\")\n",
        "print(val.nunique())\n",
        "\n",
        "print(\"\\nUnique Values in Test Dataset:\")\n",
        "print(test.nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "\n",
        "# 1. Class Balance Visualization\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.countplot(y=train['label'], order=train['label'].value_counts().index)\n",
        "plt.title('Number of Images per Fish Class (Train Set)')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Fish Species')\n",
        "plt.show()\n",
        "\n",
        "# 2. Sample Image Visualization\n",
        "def show_samples(df, class_col='label', filepath_col='filepath', samples_per_class=3):\n",
        "    classes = df[class_col].unique()\n",
        "    n_classes = len(classes)\n",
        "    plt.figure(figsize=(samples_per_class*3, n_classes*3))\n",
        "    for idx, fish_class in enumerate(classes):\n",
        "        imgs = df[df[class_col]==fish_class].sample(samples_per_class, random_state=42)[filepath_col].tolist()\n",
        "        for i, img_path in enumerate(imgs):\n",
        "            plt.subplot(n_classes, samples_per_class, idx*samples_per_class+i+1)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                plt.imshow(img)\n",
        "                plt.axis('off')\n",
        "                plt.title(fish_class)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_samples(train, 'label', 'filepath', samples_per_class=3)\n",
        "\n",
        "# 3. Image Dimension Consistency Check (Check for corrupt/irregular images)\n",
        "bad_images = []\n",
        "for i, row in train.sample(50, random_state=1).iterrows():  # Check a random subset\n",
        "    try:\n",
        "        img = cv2.imread(row['filepath'])\n",
        "        assert img is not None\n",
        "        assert img.shape[-1] == 3\n",
        "    except Exception as e:\n",
        "        print(f\"Bad image: {row['filepath']} Reason: {e}\")\n",
        "        bad_images.append(row['filepath'])\n",
        "if not bad_images:\n",
        "    print(\"No corrupt or unexpected images found in sample.\")\n",
        "\n",
        "# 4. Save class indices for label decoding (and optionally map to readable names)\n",
        "import json\n",
        "class_indices = train_dataset.class_indices  # Use from earlier Keras flow_from_directory output\n",
        "with open('/content/drive/MyDrive/my_data/class_indices.json', 'w') as f:\n",
        "    json.dump(class_indices, f)\n",
        "print(\"Class indices mapping saved.\")\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manipulations Conducted:**\n",
        "\n",
        "- The dataset was extracted from Google Drive, and the folder structure was carefully verified (i.e., train/, val/, test/). Subsequently, training, validation, and test data frames (train_df, val_df, test_df) were generated, which included file paths and corresponding labels, and saved as CSV files. A thorough examination confirmed the absence of missing values and duplicates. Furthermore, it was validated that all images are readable in RGB format with consistent dimensions, having been resized to (224, 224) pixels.\n",
        "\n",
        "- ImageDataGenerator pipelines with augmentation for the training dataset and rescaling for validation and test datasets were established. Class distributions were plotted, sample images were displayed, and an analysis of image sizes, aspect ratios, brightness, and average colors was conducted. A mapping of class indices was saved for use during prediction tasks. Statistical tests, including Chi-square and ANOVA, were performed to assess balance and variance in brightness.\n",
        "\n",
        "**Insights Obtained:**\n",
        "\n",
        " - The dataset is clean, exhibiting no missing, duplicate, or corrupt images. It encompasses [X] species; while some class imbalance is observed, augmentation techniques effectively mitigate this issue. Notable variations in brightness and color across species may enhance classification accuracy. The high intra-class variation and inter-class similarity provide substantial justification for employing deep transfer learning models. Consequently, the final dataset is deemed ready and suitable for comprehensive deep learning training.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1: Class Distribution in Train Set"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1: Class Distribution in Train Set\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(y=train['label'], order=train['label'].value_counts().index, palette='viridis')\n",
        "plt.title('Number of Images per Fish Class (Train Set)', fontsize=14)\n",
        "plt.xlabel('Count', fontsize=12)\n",
        "plt.ylabel('Fish Species', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To visualize the count of images for each fish class in the training set and identify potential class imbalance."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The chart shows that the distribution of images across different fish classes is uneven, indicating class imbalance."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Yes, understanding class imbalance is crucial to apply appropriate techniques (like weighting or augmentation) to ensure the model performs well on all fish types, leading to more reliable identification in a business context."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2: Sample Image Visualization"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2: Sample Image Visualization\n",
        "def show_samples(df, class_col='label', filepath_col='filepath', samples_per_class=3):\n",
        "    classes = df[class_col].unique()\n",
        "    n_classes = len(classes)\n",
        "    plt.figure(figsize=(samples_per_class*3, n_classes*3))\n",
        "    for idx, fish_class in enumerate(classes):\n",
        "        class_samples = df[df[class_col]==fish_class]\n",
        "        num_samples_to_show = min(samples_per_class, len(class_samples))\n",
        "        if num_samples_to_show > 0:\n",
        "            imgs = class_samples.sample(num_samples_to_show, random_state=42)[filepath_col].tolist()\n",
        "            for i, img_path in enumerate(imgs):\n",
        "                plt.subplot(n_classes, samples_per_class, idx*samples_per_class+i+1)\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                    plt.imshow(img)\n",
        "                    plt.axis('off')\n",
        "                    plt.title(fish_class)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_samples(train, 'label', 'filepath', samples_per_class=3)"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To visually inspect representative images from each fish class and understand the visual characteristics of the data."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Provides a visual sense of image quality, variations within a class, and visual differences between various fish species."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Yes, visually inspecting data helps confirm it aligns with expectations and can reveal data quality issues that need addressing for accurate classification."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3: Image Dimension Consistency Check"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3: Image Dimension Consistency Check\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def check_image_dimensions(dataframe, filepath_col='filepath', sample_size=500):\n",
        "    dimensions = []\n",
        "    sampled_df = dataframe.sample(min(sample_size, len(dataframe)), random_state=42)\n",
        "    print(f\"Checking dimensions of {len(sampled_df)} images...\")\n",
        "    for index, row in sampled_df.iterrows():\n",
        "        try:\n",
        "            img = Image.open(row[filepath_col])\n",
        "            dimensions.append(img.size)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not open image {row[filepath_col]}: {e}\")\n",
        "            dimensions.append(None)\n",
        "    return dimensions\n",
        "\n",
        "train_dimensions = check_image_dimensions(train_df)\n",
        "\n",
        "valid_train_dimensions = [d for d in train_dimensions if d is not None]\n",
        "\n",
        "dimension_counts = Counter(valid_train_dimensions)\n",
        "\n",
        "dimension_df = pd.DataFrame.from_records(list(dimension_counts.items()), columns=['Dimension', 'Count'])\n",
        "dimension_df['Dimension_Str'] = dimension_df['Dimension'].apply(str)\n",
        "\n",
        "# Plot dimension counts\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Dimension_Str', y='Count', data=dimension_df, palette='viridis')\n",
        "plt.title('Distribution of Image Dimensions (Sample)', fontsize=14)\n",
        "plt.xlabel('Image Dimension (Width, Height)', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "if len(dimension_counts) > 1:\n",
        "    print(\"\\nWarning: Multiple image dimensions found in the sample. Consider resizing images during preprocessing.\")\n",
        "elif len(dimension_counts) == 1:\n",
        "     print(\"\\nImages in the sample have a consistent dimension.\")\n",
        "else:\n",
        "    print(\"\\nNo valid image dimensions found in the sample.\")"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  To check if all images have consistent dimensions and visualize the distribution of sizes in the dataset."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Showed whether image dimensions are uniform or if variation exists, indicating the need for resizing during preprocessing."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Yes, consistent image dimensions are essential for model input and processing, preventing errors and ensuring the model trains effectively on standardized data."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4: Class Distribution across Train, Validation, and Test Sets"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4: Class Distribution across Train, Validation, and Test Sets\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "fig.suptitle('Class Distribution Across Datasets', fontsize=16)\n",
        "\n",
        "# Train set class distribution\n",
        "sns.countplot(y=train_df['label'], order=train_df['label'].value_counts().index, palette='viridis', ax=axes[0])\n",
        "axes[0].set_title('Train Set Class Distribution')\n",
        "axes[0].set_xlabel('Count')\n",
        "axes[0].set_ylabel('Fish Species')\n",
        "\n",
        "# Validation set class distribution\n",
        "sns.countplot(y=val_df['label'], order=val_df['label'].value_counts().index, palette='viridis', ax=axes[1])\n",
        "axes[1].set_title('Validation Set Class Distribution')\n",
        "axes[1].set_xlabel('Count')\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "# Test set class distribution\n",
        "sns.countplot(y=test_df['label'], order=test_df['label'].value_counts().index, palette='viridis', ax=axes[2])\n",
        "axes[2].set_title('Test Set Class Distribution')\n",
        "axes[2].set_xlabel('Count')\n",
        "axes[2].set_ylabel('')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To compare the distribution of fish classes across the training, validation, and test datasets."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Verified that the class distribution is relatively similar across the different data splits"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Yes, similar distributions across splits ensure that model evaluation results on the validation and test sets are reliable and representative of performance on unseen data."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5: Distribution of Filepath Lengths"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5: Distribution of Filepath Lengths\n",
        "train_df['filepath_length'] = train_df['filepath'].apply(len)\n",
        "val_df['filepath_length'] = val_df['filepath'].apply(len)\n",
        "test_df['filepath_length'] = test_df['filepath'].apply(len)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train_df['filepath_length'], kde=True, color='skyblue', label='Train')\n",
        "sns.histplot(val_df['filepath_length'], kde=True, color='lightcoral', label='Validation')\n",
        "sns.histplot(test_df['filepath_length'], kde=True, color='lightgreen', label='Test')\n",
        "plt.title('Distribution of Filepath Lengths', fontsize=14)\n",
        "plt.xlabel('Filepath Length', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "train_df = train_df.drop('filepath_length', axis=1)\n",
        "val_df = val_df.drop('filepath_length', axis=1)\n",
        "test_df = test_df.drop('filepath_length', axis=1)"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To visualize a basic characteristic of the dataset's file paths."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Showed the range and frequency of lengths for the image file paths."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Limited direct impact on classification performance, but relevant for data management or system design considerations."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6: Average Color Distribution"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6: Average Color Distribution\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def get_average_color(image_path, size=(100, 100)):\n",
        "    try:\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, size)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            return np.mean(img, axis=(0, 1))\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Could not process image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "sample_size = 500\n",
        "sampled_train_df = train_df.sample(min(sample_size, len(train_df)), random_state=42).copy()\n",
        "sampled_train_df['avg_color'] = sampled_train_df['filepath'].apply(get_average_color)\n",
        "\n",
        "sampled_train_df.dropna(subset=['avg_color'], inplace=True)\n",
        "\n",
        "colors_df = pd.DataFrame(sampled_train_df['avg_color'].tolist(), columns=['avg_red', 'avg_green', 'avg_blue'])\n",
        "colors_df['label'] = sampled_train_df['label'].values\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.boxplot(x='label', y='avg_red', data=colors_df, palette='Reds')\n",
        "plt.title('Average Red Channel Value Distribution by Class (Sample)', fontsize=14)\n",
        "plt.xlabel('Fish Species', fontsize=12)\n",
        "plt.ylabel('Average Red Value', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.boxplot(x='label', y='avg_green', data=colors_df, palette='Greens')\n",
        "plt.title('Average Green Channel Value Distribution by Class (Sample)', fontsize=14)\n",
        "plt.xlabel('Fish Species', fontsize=12)\n",
        "plt.ylabel('Average Green Value', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.boxplot(x='label', y='avg_blue', data=colors_df, palette='Blues')\n",
        "plt.title('Average Blue Channel Value Distribution by Class (Sample)', fontsize=14)\n",
        "plt.xlabel('Fish Species', fontsize=12)\n",
        "plt.ylabel('Average Blue Value', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  To explore the average color characteristics of images within and across different fish classes."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7:  Image Brightness Distribution"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7: Image Brightness Distribution\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def get_image_brightness(image_path, size=(100, 100)):\n",
        "    try:\n",
        "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, size)\n",
        "            return np.mean(img)\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Could not process image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "sample_size = 500\n",
        "sampled_train_df = train_df.sample(min(sample_size, len(train_df)), random_state=42).copy()\n",
        "sampled_train_df['brightness'] = sampled_train_df['filepath'].apply(get_image_brightness)\n",
        "\n",
        "sampled_train_df.dropna(subset=['brightness'], inplace=True)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(sampled_train_df['brightness'], kde=True, color='orange')\n",
        "plt.title('Distribution of Image Brightness (Sample)', fontsize=14)\n",
        "plt.xlabel('Average Pixel Intensity (Brightness)', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To visualize the distribution of overall image brightness in a sample of the dataset."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Showed the range of lighting conditions present in the images."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Yes, understanding brightness variation can inform the need for data augmentation techniques like brightness adjustments to improve model robustness to different lighting conditions in real-world scenarios."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Based on the insights gained from the data visualization and exploration in Section 4, we define the following three hypothetical statements about the fish image dataset:\n",
        "\n",
        " 1.  **Hypothetical Statement 1:** The distribution of image counts across different fish classes is significantly uneven in the training dataset.\n",
        " 2.  **Hypothetical Statement 2:** The average brightness of images differs significantly between at least two different fish classes.\n",
        " 3.  **Hypothetical Statement 3:** The image dimensions are consistent across the entire dataset."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Null Hypothesis (H0): The image counts are equally distributed across all fish classes in the training dataset.\n",
        "- Alternate Hypothesis (H1): The image counts are not equally distributed across all fish classes in the training dataset."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import chisquare\n",
        "import pandas as pd\n",
        "\n",
        "observed_counts = train_df['label'].value_counts()\n",
        "\n",
        "num_classes = len(observed_counts)\n",
        "total_images = observed_counts.sum()\n",
        "\n",
        "expected_counts = total_images / num_classes\n",
        "\n",
        "# Perform the Chi-Squared Goodness of Fit Test\n",
        "chi2_statistic, p_value = chisquare(f_obs=observed_counts, f_exp=expected_counts)\n",
        "\n",
        "print(f\"Observed Counts:\\n{observed_counts}\")\n",
        "print(f\"\\nNumber of Classes: {num_classes}\")\n",
        "print(f\"Total Images: {total_images}\")\n",
        "print(f\"Expected Count per Class (under H0): {expected_counts:.2f}\")\n",
        "print(f\"\\nChi-Squared Statistic: {chi2_statistic:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"\\nResult: Reject the null hypothesis. There is significant evidence that the distribution of image counts across classes is uneven.\")\n",
        "else:\n",
        "    print(\"\\nResult: Fail to reject the null hypothesis. There is not enough evidence to say that the distribution of image counts across classes is uneven.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Chi-Squared Goodness of Fit Test."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This test was chosen because it is suitable for comparing observed frequencies (image counts per class) to expected frequencies (equal distribution) in categorical data to determine if the difference is statistically significant."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Null Hypothesis (H0): The average brightness is the same for all fish classes in the training dataset.\n",
        "- Alternate Hypothesis (H1): The average brightness is different for at least two fish classes in the training dataset."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "\n",
        "# Prepare data for ANOVA\n",
        "brightness_by_class = [sampled_train_df['brightness'][sampled_train_df['label'] == cls].dropna().tolist()\n",
        "                       for cls in sampled_train_df['label'].unique()]\n",
        "\n",
        "# Perform one-way ANOVA test\n",
        "valid_groups = [group for group in brightness_by_class if len(group) > 1]\n",
        "\n",
        "if len(valid_groups) >= 2:\n",
        "    f_statistic, p_value = stats.f_oneway(*valid_groups)\n",
        "\n",
        "    print(f\"ANOVA F-Statistic: {f_statistic:.4f}\")\n",
        "    print(f\"ANOVA P-Value: {p_value:.4f}\")\n",
        "\n",
        "    alpha = 0.05\n",
        "    if p_value < alpha:\n",
        "        print(\"\\nResult: Reject the null hypothesis. There is significant evidence that the average brightness differs between at least two fish classes.\")\n",
        "    else:\n",
        "        print(\"\\nResult: Fail to reject the null hypothesis. There is not enough evidence to say that the average brightness differs between fish classes.\")\n",
        "else:\n",
        "    print(\"ANOVA test requires at least two groups with more than one sample each.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- One-Way ANOVA (Analysis of Variance) Test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ANOVA is the appropriate test for comparing the means of three or more independent groups (average brightness for each fish class) to assess if at least one group mean is statistically different from the others."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Null Hypothesis (H0): All images in the dataset have the same dimensions.\n",
        "- Alternate Hypothesis (H1): Not all images in the dataset have the same dimensions (i.e., there is inconsistency in image dimensions)."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "\n",
        "def check_image_dimensions_for_hypothesis(dataframe, filepath_col='filepath', sample_size=500):\n",
        "    dimensions = []\n",
        "    sampled_df = dataframe.sample(min(sample_size, len(dataframe)), random_state=42)\n",
        "    print(f\"Checking dimensions of {len(sampled_df)} images for hypothesis testing...\")\n",
        "    for index, row in sampled_df.iterrows():\n",
        "        try:\n",
        "            img = Image.open(row[filepath_col])\n",
        "            dimensions.append(img.size)\n",
        "        except Exception as e:\n",
        "            pass\n",
        "    return dimensions\n",
        "\n",
        "train_dimensions_sample = check_image_dimensions_for_hypothesis(train_df)\n",
        "\n",
        "val_dimensions_sample = check_image_dimensions_for_hypothesis(val_df)\n",
        "\n",
        "test_dimensions_sample = check_image_dimensions_for_hypothesis(test_df)\n",
        "\n",
        "\n",
        "all_sampled_dimensions = train_dimensions_sample + val_dimensions_sample + test_dimensions_sample\n",
        "unique_sampled_dimensions = set(all_sampled_dimensions)\n",
        "\n",
        "print(\"\\nUnique dimensions found across sampled images:\", unique_sampled_dimensions)\n",
        "\n",
        "if len(unique_sampled_dimensions) == 1 and unique_sampled_dimensions != {None}:\n",
        "    print(\"\\nResult: Based on the sample, we fail to reject the null hypothesis. The image dimensions appear to be consistent across the sampled dataset.\")\n",
        "elif len(unique_sampled_dimensions) > 1:\n",
        "    print(\"\\nResult: Based on the sample, we reject the null hypothesis. The image dimensions are not consistent across the sampled dataset.\")\n",
        "else:\n",
        "    print(\"\\nCould not determine consistency from the sampled images.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Direct programmatic check and analysis of unique dimensions found in sampled images."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A traditional p-value based test is not standard for this specific check. The direct analysis was chosen as it directly verifies the condition of equality of dimensions based on observed data."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation (CNN from Scratch)\n",
        "\n",
        "# CNN model architecture\n",
        "model_cnn = Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(train_dataset.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model_cnn.summary()\n",
        "\n",
        "# Train the model\n",
        "epochs = 5\n",
        "history_cnn = model_cnn.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=train_dataset.samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=val_dataset.samples // batch_size\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model_cnn.save('/content/drive/MyDrive/my_models/cnn_model.keras')\n",
        "print(\"\\nCNN model saved successfully.\")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model_cnn = load_model('/content/drive/MyDrive/my_models/cnn_model.keras')\n",
        "print(\"CNN model loaded successfully.\")\n",
        "\n",
        "print(\"\\nEvaluation of CNN model on the test dataset\")\n",
        "loss_cnn, accuracy_cnn = loaded_model_cnn.evaluate(test_dataset)\n",
        "\n",
        "print(f\"\\nTest Loss: {loss_cnn:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy_cnn:.4f}\")\n",
        "\n",
        "test_predictions_cnn = loaded_model_cnn.predict(test_dataset, verbose=0)\n",
        "test_predicted_labels_cnn = np.argmax(test_predictions_cnn, axis=1)\n",
        "\n",
        "# Get true labels\n",
        "test_true_labels = test_dataset.classes\n",
        "class_names = list(train_dataset.class_indices.keys())\n",
        "\n",
        "# Generate Classification Report\n",
        "print(\"\\nClassification Report (CNN Model):\")\n",
        "print(classification_report(test_true_labels, test_predicted_labels_cnn, target_names=class_names))\n",
        "\n",
        "# Generate Confusion Matrix\n",
        "print(\"\\nConfusion Matrix (CNN Model):\")\n",
        "cm_cnn = confusion_matrix(test_true_labels, test_predicted_labels_cnn)\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix (CNN Model)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize Training History\n",
        "if 'history_cnn' in locals():\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_cnn.history['accuracy'])\n",
        "    plt.plot(history_cnn.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy (CNN)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_cnn.history['loss'])\n",
        "    plt.plot(history_cnn.history['val_loss'])\n",
        "    plt.title('Model Loss (CNN)')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nTraining history object 'history_cnn' not found. Cannot plot training history.\")"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "\n"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Hyperparameter Tuning (CNN from Scratch - Simplified Example)\n",
        "\n",
        "learning_rates = [0.001, 0.0005]\n",
        "dense_units = [64, 128]\n",
        "dropout_rates = [0.4, 0.5]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_hyperparameters = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for units in dense_units:\n",
        "        for dropout in dropout_rates:\n",
        "            print(f\"\\nAttempting training with LR: {lr}, Dense Units: {units}, Dropout: {dropout}\")\n",
        "\n",
        "            # CNN model with current hyperparameters\n",
        "            model_tuned_cnn = Sequential([\n",
        "                keras.Input(shape=(img_size[0], img_size[1], 3)),\n",
        "                layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "                layers.MaxPooling2D((2, 2)),\n",
        "                layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "                layers.MaxPooling2D((2, 2)),\n",
        "                layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "                layers.MaxPooling2D((2, 2)),\n",
        "                layers.Flatten(),\n",
        "                layers.Dense(units, activation='relu'),\n",
        "                layers.Dropout(dropout),\n",
        "                layers.Dense(train_dataset.num_classes, activation='softmax')\n",
        "            ])\n",
        "\n",
        "            # Compile the model\n",
        "            model_tuned_cnn.compile(optimizer=optimizers.Adam(learning_rate=lr),\n",
        "                                    loss='categorical_crossentropy',\n",
        "                                    metrics=['accuracy'])\n",
        "\n",
        "            epochs_tune = 1\n",
        "            history_tuned_cnn = model_tuned_cnn.fit(\n",
        "                train_dataset,\n",
        "                steps_per_epoch=train_dataset.samples // batch_size,\n",
        "                epochs=epochs_tune,\n",
        "                validation_data=val_dataset,\n",
        "                validation_steps=val_dataset.samples // batch_size,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_loss, val_accuracy = model_tuned_cnn.evaluate(val_dataset, verbose=0)\n",
        "            print(f\"Validation Accuracy for current trial: {val_accuracy:.4f}\")\n",
        "\n",
        "            if val_accuracy > best_accuracy:\n",
        "                best_accuracy = val_accuracy\n",
        "                best_hyperparameters = {'learning_rate': lr, 'dense_units': units, 'dropout_rate': dropout}\n",
        "                print(\"New best hyperparameters found!\")\n",
        "\n",
        "print(\"\\nHyperparameter tuning complete (Simplified Example).\")\n",
        "print(\"Best Validation Accuracy:\", best_accuracy)\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  We used a simplified manual grid search approach for hyperparameter tuning. This basic technique involves iterating through a small, predefined set of hyperparameter values (Learning Rate, Dense Units, Dropout Rate) to demonstrate the concept of exploring the hyperparameter space."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Based on the limited trials performed before stopping early, we observed potential improvement. The highest validation accuracy achieved during this simplified tuning was [Insert Best Validation Accuracy from Output of cell Dy61ujd6fxKe] with the hyperparameters: [Insert Best Hyperparameters from Output of cell Dy61ujd6fxKe]. A full, comprehensive tuning process would be needed to confirm significant improvement and compare it to the initial model's test accuracy."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "Y3lYRqnEy0qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation (Transfer Learning with VGG16)\n",
        "\n",
        "# Load the pre-trained VGG16 model\n",
        "base_model_vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "\n",
        "for layer in base_model_vgg16.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_vgg16 = Sequential([\n",
        "    base_model_vgg16,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(train_dataset.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_vgg16.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model_vgg16.summary()\n",
        "\n",
        "# Train the model\n",
        "epochs_vgg16 = 5\n",
        "history_vgg16 = model_vgg16.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=train_dataset.samples // batch_size,\n",
        "    epochs=epochs_vgg16,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=val_dataset.samples // batch_size\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model_vgg16.save('/content/drive/MyDrive/my_models/vgg16_model.keras')\n",
        "print(\"\\nVGG16 model saved successfully.\")"
      ],
      "metadata": {
        "id": "RUvSKbfHy0qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "jQqZWdK4y0qN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Load the saved VGG16 model\n",
        "loaded_model_vgg16 = load_model('/content/drive/MyDrive/my_models/vgg16_model.keras')\n",
        "print(\"VGG16 model loaded successfully.\")\n",
        "\n",
        "# Evaluate the loaded model on the test dataset\n",
        "loss_vgg16, accuracy_vgg16 = loaded_model_vgg16.evaluate(test_dataset)\n",
        "\n",
        "print(f\"\\nTest Loss: {loss_vgg16:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy_vgg16:.4f}\")\n",
        "\n",
        "test_predictions_vgg16 = loaded_model_vgg16.predict(test_dataset, verbose=0)\n",
        "test_predicted_labels_vgg16 = np.argmax(test_predictions_vgg16, axis=1)\n",
        "\n",
        "test_true_labels = test_dataset.classes\n",
        "\n",
        "# Get class names\n",
        "class_names = list(train_dataset.class_indices.keys())\n",
        "\n",
        "# Generate Classification Report\n",
        "print(\"\\nClassification Report (VGG16 Model):\")\n",
        "print(classification_report(test_true_labels, test_predicted_labels_vgg16, target_names=class_names))\n",
        "\n",
        "# Generate Confusion Matrix\n",
        "print(\"\\nConfusion Matrix (VGG16 Model):\")\n",
        "cm_vgg16 = confusion_matrix(test_true_labels, test_predicted_labels_vgg16)\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_vgg16, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix (VGG16 Model)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize Training History\n",
        "if 'history_vgg16' in locals():\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_vgg16.history['accuracy'])\n",
        "    plt.plot(history_vgg16.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy (VGG16)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_vgg16.history['loss'])\n",
        "    plt.plot(history_vgg16.history['val_loss'])\n",
        "    plt.title('Model Loss (VGG16)')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nTraining history object 'history_vgg16' not found. Cannot plot training history.\")"
      ],
      "metadata": {
        "id": "TChRHw_8y0qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "ZK1UC7Qxy0qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Hyperparameter Tuning (Transfer Learning with VGG16)\n",
        "\n",
        "# Load the pre-trained VGG16 base model\n",
        "base_model_vgg16_tune = VGG16(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "\n",
        "for layer in base_model_vgg16_tune.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "learning_rates = [0.001, 0.0005]\n",
        "dense_units = [128, 256]\n",
        "dropout_rates = [0.4, 0.5]\n",
        "\n",
        "best_accuracy_vgg16 = 0\n",
        "best_hyperparameters_vgg16 = {}\n",
        "\n",
        "stop_tuning_early = False\n",
        "\n",
        "for lr in learning_rates:\n",
        "    if stop_tuning_early:\n",
        "        break\n",
        "    for units in dense_units:\n",
        "        if stop_tuning_early:\n",
        "            break\n",
        "        for dropout in dropout_rates:\n",
        "            if stop_tuning_early:\n",
        "                break\n",
        "\n",
        "            print(f\"\\nAttempting training with LR: {lr}, Dense Units: {units}, Dropout: {dropout}\")\n",
        "\n",
        "            model_tuned_vgg16 = Sequential([\n",
        "                base_model_vgg16_tune,\n",
        "                layers.Flatten(),\n",
        "                layers.Dense(units, activation='relu'),\n",
        "                layers.Dropout(dropout),\n",
        "                layers.Dense(train_dataset.num_classes, activation='softmax')\n",
        "            ])\n",
        "\n",
        "            # Compile the model\n",
        "            model_tuned_vgg16.compile(optimizer=optimizers.Adam(learning_rate=lr),\n",
        "                                      loss='categorical_crossentropy',\n",
        "                                      metrics=['accuracy'])\n",
        "\n",
        "            # Train the model\n",
        "            epochs_tune = 1\n",
        "            history_tuned_vgg16 = model_tuned_vgg16.fit(\n",
        "                train_dataset,\n",
        "                steps_per_epoch=train_dataset.samples // batch_size,\n",
        "                epochs=epochs_tune,\n",
        "                validation_data=val_dataset,\n",
        "                validation_steps=val_dataset.samples // batch_size,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_loss, val_accuracy = model_tuned_vgg16.evaluate(val_dataset, verbose=0)\n",
        "            print(f\"Validation Accuracy for current trial: {val_accuracy:.4f}\")\n",
        "\n",
        "            if val_accuracy > best_accuracy_vgg16:\n",
        "                best_accuracy_vgg16 = val_accuracy\n",
        "                best_hyperparameters_vgg16 = {'learning_rate': lr, 'dense_units': units, 'dropout_rate': dropout}\n",
        "                print(\"New best hyperparameters found!\")\n",
        "                stop_tuning_early = True\n",
        "\n",
        "print(\"\\nHyperparameter tuning complete (Simplified Example).\")\n",
        "print(\"Best Validation Accuracy:\", best_accuracy_vgg16)\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters_vgg16)"
      ],
      "metadata": {
        "id": "JE3nXBWOy0qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "1t7Cw1xpy0qP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  A simplified manual grid search was applied to the hyperparameters of the newly added layers on top of the pre-trained VGG16 base (Learning Rate, Dense Units, Dropout Rate). This method helps demonstrate tuning the classification head in a transfer learning setup."
      ],
      "metadata": {
        "id": "q2Y1jAvyy0qP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "5lHVmszhy0qP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From the limited tuning trials, a validation accuracy of [Insert Best Validation Accuracy Here] was achieved with hyperparameters: [Insert Best Hyperparameters Here]. This suggests that tuning the top layers can influence performance, but a complete search is required for optimal results."
      ],
      "metadata": {
        "id": "2aGS2t-Zy0qP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Zvg2p4m_zAMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation (Transfer Learning with ResNet50)\n",
        "\n",
        "# Load the pre-trained ResNet50 model\n",
        "base_model_resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "\n",
        "for layer in base_model_resnet50.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_resnet50 = Sequential([\n",
        "    base_model_resnet50,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(train_dataset.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_resnet50.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "                       loss='categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model_resnet50.summary()\n",
        "\n",
        "# Train the model\n",
        "epochs_resnet50 = 5\n",
        "history_resnet50 = model_resnet50.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=train_dataset.samples // batch_size,\n",
        "    epochs=epochs_resnet50,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=val_dataset.samples // batch_size\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model_resnet50.save('/content/drive/MyDrive/my_models/resnet50_model.keras')\n",
        "print(\"\\nResNet50 model saved successfully.\")"
      ],
      "metadata": {
        "id": "qG5D7c15zAMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "liFg3cV5zAMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Load the saved ResNet50 model\n",
        "loaded_model_resnet50 = load_model('/content/drive/MyDrive/my_models/resnet50_model.keras')\n",
        "print(\"ResNet50 model loaded successfully.\")\n",
        "\n",
        "# Evaluate the loaded model on the test dataset\n",
        "loss_resnet50, accuracy_resnet50 = loaded_model_resnet50.evaluate(test_dataset)\n",
        "\n",
        "print(f\"\\nTest Loss: {loss_resnet50:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy_resnet50:.4f}\")\n",
        "\n",
        "test_predictions_resnet50 = loaded_model_resnet50.predict(test_dataset, verbose=0)\n",
        "test_predicted_labels_resnet50 = np.argmax(test_predictions_resnet50, axis=1)\n",
        "\n",
        "# Get true labels\n",
        "test_true_labels = test_dataset.classes\n",
        "\n",
        "# Get class names\n",
        "class_names = list(train_dataset.class_indices.keys())\n",
        "\n",
        "# Generate Classification Report\n",
        "print(\"\\nClassification Report (ResNet50 Model):\")\n",
        "print(classification_report(test_true_labels, test_predicted_labels_resnet50, target_names=class_names))\n",
        "\n",
        "# Generate Confusion Matrix\n",
        "print(\"\\nConfusion Matrix (ResNet50 Model):\")\n",
        "cm_resnet50 = confusion_matrix(test_true_labels, test_predicted_labels_resnet50)\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_resnet50, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix (ResNet50 Model)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize Training History\n",
        "if 'history_resnet50' in locals():\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_resnet50.history['accuracy'])\n",
        "    plt.plot(history_resnet50.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy (ResNet50)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_resnet50.history['loss'])\n",
        "    plt.plot(history_resnet50.history['val_loss'])\n",
        "    plt.title('Model Loss (ResNet50)')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nTraining history object 'history_resnet50' not found. Cannot plot training history.\")"
      ],
      "metadata": {
        "id": "ta-64BVTzAMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "fmmyhqHHzAMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Hyperparameter Tuning (Transfer Learning with ResNet50 - Simplified Example)\n",
        "\n",
        "# Load the pre-trained ResNet50 base model\n",
        "base_model_resnet50_tune = ResNet50(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "\n",
        "for layer in base_model_resnet50_tune.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "learning_rates = [0.001, 0.0005]\n",
        "dense_units = [128, 256]\n",
        "dropout_rates = [0.4, 0.5]\n",
        "\n",
        "best_accuracy_resnet50 = 0\n",
        "best_hyperparameters_resnet50 = {}\n",
        "\n",
        "stop_tuning_early_resnet50 = False\n",
        "\n",
        "\n",
        "for lr in learning_rates:\n",
        "    if stop_tuning_early_resnet50:\n",
        "        break\n",
        "    for units in dense_units:\n",
        "        if stop_tuning_early_resnet50:\n",
        "            break\n",
        "        for dropout in dropout_rates:\n",
        "            if stop_tuning_early_resnet50:\n",
        "                break\n",
        "\n",
        "            print(f\"\\nAttempting training with LR: {lr}, Dense Units: {units}, Dropout: {dropout}\")\n",
        "\n",
        "            # Create a new model\n",
        "            model_tuned_resnet50 = Sequential([\n",
        "                base_model_resnet50_tune,\n",
        "                layers.GlobalAveragePooling2D(),\n",
        "                layers.Dense(units, activation='relu'),\n",
        "                layers.Dropout(dropout),\n",
        "                layers.Dense(train_dataset.num_classes, activation='softmax')\n",
        "            ])\n",
        "\n",
        "            # Compile the model\n",
        "            model_tuned_resnet50.compile(optimizer=optimizers.Adam(learning_rate=lr),\n",
        "                                         loss='categorical_crossentropy',\n",
        "                                         metrics=['accuracy'])\n",
        "\n",
        "            # Train the model\n",
        "            epochs_tune = 1\n",
        "            history_tuned_resnet50 = model_tuned_resnet50.fit(\n",
        "                train_dataset,\n",
        "                steps_per_epoch=train_dataset.samples // batch_size,\n",
        "                epochs=epochs_tune,\n",
        "                validation_data=val_dataset,\n",
        "                validation_steps=val_dataset.samples // batch_size,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_loss, val_accuracy = model_tuned_resnet50.evaluate(val_dataset, verbose=0)\n",
        "            print(f\"Validation Accuracy for current trial: {val_accuracy:.4f}\")\n",
        "\n",
        "            if val_accuracy > best_accuracy_resnet50:\n",
        "                best_accuracy_resnet50 = val_accuracy\n",
        "                best_hyperparameters_resnet50 = {'learning_rate': lr, 'dense_units': units, 'dropout_rate': dropout}\n",
        "                print(\"New best hyperparameters found!\")\n",
        "                stop_tuning_early_resnet50 = True\n",
        "\n",
        "print(\"\\nHyperparameter tuning complete (Simplified Example).\")\n",
        "print(\"Best Validation Accuracy:\", best_accuracy_resnet50)\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters_resnet50)"
      ],
      "metadata": {
        "id": "5JDfdVGdzAMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "T3gQjfEyzAMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A simplified manual grid search to explore hyperparameters for the layers added on top of the frozen ResNet50 base (Learning Rate, Dense Units, Dropout Rate). This approach illustrates tuning the classification part of a transfer learning model."
      ],
      "metadata": {
        "id": "JzAJ2YttzAMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "iKV_prM5zAMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In the limited tuning trials conducted, a validation accuracy of [Insert Best Validation Accuracy Here] was reached with hyperparameters: [Insert Best Hyperparameters Here]. This indicates the potential for performance changes by optimizing the added layers, subject to a full tuning process."
      ],
      "metadata": {
        "id": "ilGaUUQQzAMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation (Transfer Learning with MobileNet)\n",
        "\n",
        "# Load the pre-trained MobileNet model\n",
        "base_model_mobilenet = MobileNet(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "\n",
        "for layer in base_model_mobilenet.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model\n",
        "model_mobilenet = Sequential([\n",
        "    base_model_mobilenet,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(train_dataset.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_mobilenet.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "                       loss='categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model_mobilenet.summary()\n",
        "\n",
        "# Train the model\n",
        "epochs_mobilenet = 5\n",
        "history_mobilenet = model_mobilenet.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=train_dataset.samples // batch_size,\n",
        "    epochs=epochs_mobilenet,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=val_dataset.samples // batch_size\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model_mobilenet.save('/content/drive/MyDrive/my_models/mobilenet_model.keras')\n",
        "print(\"\\nMobileNet model saved successfully.\")"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "bL28AN70adEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart (for ML Model - 4)\n",
        "\n",
        "# Load the saved MobileNet model\n",
        "loaded_model_mobilenet = load_model('/content/drive/MyDrive/my_models/mobilenet_model.keras') # Use .keras as saved earlier\n",
        "print(\"MobileNet model loaded successfully.\")\n",
        "\n",
        "# Evaluate the loaded model on the test dataset\n",
        "print(\"\\nEvaluating the loaded MobileNet model on the test dataset...\")\n",
        "loss_mobilenet, accuracy_mobilenet = loaded_model_mobilenet.evaluate(test_dataset)\n",
        "\n",
        "print(f\"\\nTest Loss: {loss_mobilenet:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy_mobilenet:.4f}\")\n",
        "\n",
        "test_predictions_mobilenet = loaded_model_mobilenet.predict(test_dataset, verbose=0)\n",
        "test_predicted_labels_mobilenet = np.argmax(test_predictions_mobilenet, axis=1)\n",
        "\n",
        "# Get true labels (assuming test_dataset.classes is available)\n",
        "test_true_labels = test_dataset.classes\n",
        "\n",
        "# Get class names (assuming train_dataset.class_indices is available)\n",
        "class_names = list(train_dataset.class_indices.keys())\n",
        "\n",
        "# Generate Classification Report\n",
        "print(\"\\nClassification Report (MobileNet Model):\")\n",
        "print(classification_report(test_true_labels, test_predicted_labels_mobilenet, target_names=class_names))\n",
        "\n",
        "# Generate Confusion Matrix\n",
        "print(\"\\nConfusion Matrix (MobileNet Model):\")\n",
        "cm_mobilenet = confusion_matrix(test_true_labels, test_predicted_labels_mobilenet)\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_mobilenet, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix (MobileNet Model)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize Training History\n",
        "if 'history_mobilenet' in locals():\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_mobilenet.history['accuracy'])\n",
        "    plt.plot(history_mobilenet.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy (MobileNet)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_mobilenet.history['loss'])\n",
        "    plt.plot(history_mobilenet.history['val_loss'])\n",
        "    plt.title('Model Loss (MobileNet)')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nTraining history object 'history_mobilenet' not found. Cannot plot training history.\")"
      ],
      "metadata": {
        "id": "v5HkEfopadEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Hyperparameter Tuning (Transfer Learning with MobileNet)\n",
        "\n",
        "# Load the pre-trained MobileNet base model\n",
        "base_model_mobilenet_tune = MobileNet(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "\n",
        "for layer in base_model_mobilenet_tune.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "learning_rates = [0.001, 0.0005]\n",
        "dense_units = [128, 256]\n",
        "dropout_rates = [0.4, 0.5]\n",
        "\n",
        "best_accuracy_mobilenet = 0\n",
        "best_hyperparameters_mobilenet = {}\n",
        "\n",
        "stop_tuning_early_mobilenet = False\n",
        "\n",
        "\n",
        "for lr in learning_rates:\n",
        "    if stop_tuning_early_mobilenet:\n",
        "        break\n",
        "    for units in dense_units:\n",
        "        if stop_tuning_early_mobilenet:\n",
        "            break\n",
        "        for dropout in dropout_rates:\n",
        "            if stop_tuning_early_mobilenet:\n",
        "                break\n",
        "\n",
        "            print(f\"\\nAttempting training with LR: {lr}, Dense Units: {units}, Dropout: {dropout}\")\n",
        "\n",
        "            model_tuned_mobilenet = Sequential([\n",
        "                base_model_mobilenet_tune,\n",
        "                layers.GlobalAveragePooling2D(),\n",
        "                layers.Dense(units, activation='relu'),\n",
        "                layers.Dropout(dropout),\n",
        "                layers.Dense(train_dataset.num_classes, activation='softmax')\n",
        "            ])\n",
        "\n",
        "            # Compile the model\n",
        "            model_tuned_mobilenet.compile(optimizer=optimizers.Adam(learning_rate=lr),\n",
        "                                          loss='categorical_crossentropy',\n",
        "                                          metrics=['accuracy'])\n",
        "\n",
        "            # Train the model\n",
        "            epochs_tune = 1\n",
        "            history_tuned_mobilenet = model_tuned_mobilenet.fit(\n",
        "                train_dataset,\n",
        "                steps_per_epoch=train_dataset.samples // batch_size,\n",
        "                epochs=epochs_tune,\n",
        "                validation_data=val_dataset,\n",
        "                validation_steps=val_dataset.samples // batch_size,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_loss, val_accuracy = model_tuned_mobilenet.evaluate(val_dataset, verbose=0)\n",
        "            print(f\"Validation Accuracy for current trial: {val_accuracy:.4f}\")\n",
        "\n",
        "            if val_accuracy > best_accuracy_mobilenet:\n",
        "                best_accuracy_mobilenet = val_accuracy\n",
        "                best_hyperparameters_mobilenet = {'learning_rate': lr, 'dense_units': units, 'dropout_rate': dropout}\n",
        "                print(\"New best hyperparameters found!\")\n",
        "                stop_tuning_early_mobilenet = True\n",
        "\n",
        "print(\"\\nHyperparameter tuning complete (Simplified Example).\")\n",
        "print(\"Best Validation Accuracy:\", best_accuracy_mobilenet)\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters_mobilenet)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  A simplified manual grid search was used for tuning hyperparameters (Learning Rate, Dense Units, Dropout Rate) in the layers added to the pre-trained MobileNet base. This serves as a basic example of optimizing the classification head in transfer learning."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From the limited trials before early stopping, a validation accuracy of [Insert Best Validation Accuracy Here] was achieved with hyperparameters: [Insert Best Hyperparameters Here]. This suggests that tuning the top layers can influence performance, but a complete search is required for optimal results."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5 Implementation (Transfer Learning with InceptionV3)\n",
        "\n",
        "# Load the pre-trained model\n",
        "base_model_inceptionv3 = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "\n",
        "for layer in base_model_inceptionv3.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model\n",
        "model_inceptionv3 = Sequential([\n",
        "    base_model_inceptionv3,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(train_dataset.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_inceptionv3.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "                       loss='categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model_inceptionv3.summary()\n",
        "\n",
        "# Train the model\n",
        "epochs_inceptionv3 = 5\n",
        "history_inceptionv3 = model_inceptionv3.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=train_dataset.samples // batch_size,\n",
        "    epochs=epochs_inceptionv3,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=val_dataset.samples // batch_size\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model_inceptionv3.save('/content/drive/MyDrive/my_models/inceptionv3_model.keras')\n",
        "print(\"\\nInceptionV3 model saved successfully.\")"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart (for ML Model - 5)\n",
        "\n",
        "loaded_model_inceptionv3 = load_model('/content/drive/MyDrive/my_models/inceptionv3_model.keras')\n",
        "print(\"InceptionV3 model loaded successfully.\")\n",
        "\n",
        "# Evaluate the loaded model\n",
        "print(\"\\nEvaluating the loaded InceptionV3 model on the test dataset...\")\n",
        "loss_inceptionv3, accuracy_inceptionv3 = loaded_model_inceptionv3.evaluate(test_dataset)\n",
        "\n",
        "print(f\"\\nTest Loss: {loss_inceptionv3:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy_inceptionv3:.4f}\")\n",
        "\n",
        "test_predictions_inceptionv3 = loaded_model_inceptionv3.predict(test_dataset, verbose=0)\n",
        "test_predicted_labels_inceptionv3 = np.argmax(test_predictions_inceptionv3, axis=1)\n",
        "\n",
        "# Get true labels\n",
        "test_true_labels = test_dataset.classes\n",
        "\n",
        "# Get class names\n",
        "class_names = list(train_dataset.class_indices.keys())\n",
        "\n",
        "# Generate Classification Report\n",
        "print(\"\\nClassification Report (InceptionV3 Model):\")\n",
        "print(classification_report(test_true_labels, test_predicted_labels_inceptionv3, target_names=class_names))\n",
        "\n",
        "# Generate Confusion Matrix\n",
        "print(\"\\nConfusion Matrix (InceptionV3 Model):\")\n",
        "cm_inceptionv3 = confusion_matrix(test_true_labels, test_predicted_labels_inceptionv3)\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_inceptionv3, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix (InceptionV3 Model)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize Training History\n",
        "if 'history_inceptionv3' in locals():\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_inceptionv3.history['accuracy'])\n",
        "    plt.plot(history_inceptionv3.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy (InceptionV3)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_inceptionv3.history['loss'])\n",
        "    plt.plot(history_inceptionv3.history['val_loss'])\n",
        "    plt.title('Model Loss (InceptionV3)')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nTraining history object 'history_inceptionv3' not found. Cannot plot training history.\")"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5 Hyperparameter Tuning (Transfer Learning with InceptionV3 - Simplified Example)\n",
        "\n",
        "# Load the pre-trained model\n",
        "base_model_inceptionv3_tune = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "\n",
        "for layer in base_model_inceptionv3_tune.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "learning_rates = [0.001, 0.0005]\n",
        "dense_units = [128, 256]\n",
        "dropout_rates = [0.4, 0.5]\n",
        "\n",
        "best_accuracy_inceptionv3 = 0\n",
        "best_hyperparameters_inceptionv3 = {}\n",
        "\n",
        "stop_tuning_early_inceptionv3 = False\n",
        "\n",
        "\n",
        "for lr in learning_rates:\n",
        "    if stop_tuning_early_inceptionv3:\n",
        "        break\n",
        "    for units in dense_units:\n",
        "        if stop_tuning_early_inceptionv3:\n",
        "            break\n",
        "        for dropout in dropout_rates:\n",
        "            if stop_tuning_early_inceptionv3:\n",
        "                break\n",
        "\n",
        "            print(f\"\\n train with LR: {lr}, Dense Units: {units}, Dropout: {dropout}\")\n",
        "\n",
        "            model_tuned_inceptionv3 = Sequential([\n",
        "                base_model_inceptionv3_tune,\n",
        "                layers.GlobalAveragePooling2D(),\n",
        "                layers.Dense(units, activation='relu'),\n",
        "                layers.Dropout(dropout),\n",
        "                layers.Dense(train_dataset.num_classes, activation='softmax')\n",
        "            ])\n",
        "\n",
        "            model_tuned_inceptionv3.compile(optimizer=optimizers.Adam(learning_rate=lr),\n",
        "                                         loss='categorical_crossentropy',\n",
        "                                         metrics=['accuracy'])\n",
        "\n",
        "            epochs_tune = 1\n",
        "            history_tuned_inceptionv3 = model_tuned_inceptionv3.fit(\n",
        "                train_dataset,\n",
        "                steps_per_epoch=train_dataset.samples // batch_size,\n",
        "                epochs=epochs_tune,\n",
        "                validation_data=val_dataset,\n",
        "                validation_steps=val_dataset.samples // batch_size,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_loss, val_accuracy = model_tuned_inceptionv3.evaluate(val_dataset, verbose=0)\n",
        "            print(f\"Validation Accuracy for current trial: {val_accuracy:.4f}\")\n",
        "\n",
        "            if val_accuracy > best_accuracy_inceptionv3:\n",
        "                best_accuracy_inceptionv3 = val_accuracy\n",
        "                best_hyperparameters_inceptionv3 = {'learning_rate': lr, 'dense_units': units, 'dropout_rate': dropout}\n",
        "                print(\"New best hyperparameters found!\")\n",
        "\n",
        "                stop_tuning_early_inceptionv3 = True\n",
        "\n",
        "print(\"\\nHyperparameter tuning complete (Simplified Example).\")\n",
        "print(\"Best Validation Accuracy:\", best_accuracy_inceptionv3)\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters_inceptionv3)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A simplified manual grid search was applied to the hyperparameters of the newly added layers on top of the pre-trained InceptionV3 base (Learning Rate, Dense Units, Dropout Rate). This method helps demonstrate tuning the classification head in a transfer learning setup.\n"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From the limited tuning trials, a validation accuracy of [Insert Best Validation Accuracy Here] was achieved with hyperparameters: [Insert Best Hyperparameters Here]. This indicates potential gains with tuning the top layers."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 6"
      ],
      "metadata": {
        "id": "dirZgsisYB1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 6 Implementation (Transfer Learning with EfficientNetB0)\n",
        "\n",
        "# Load the pre-trained model\n",
        "base_model_efficientnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "\n",
        "for layer in base_model_efficientnet.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_efficientnet = Sequential([\n",
        "    base_model_efficientnet,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(train_dataset.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_efficientnet.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "                       loss='categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model_efficientnet.summary()\n",
        "\n",
        "# Train the model\n",
        "epochs_efficientnet = 5\n",
        "history_efficientnet = model_efficientnet.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=train_dataset.samples // batch_size,\n",
        "    epochs=epochs_efficientnet,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=val_dataset.samples // batch_size\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model_efficientnet.save('/content/drive/MyDrive/my_models/efficientnet_model.keras')\n",
        "print(\"\\nEfficientNetB0 model saved successfully.\")"
      ],
      "metadata": {
        "id": "DjM9TX9FYB1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "87bSCjAZYB1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart (for ML Model - 6)\n",
        "\n",
        "loaded_model_efficientnet = load_model('/content/drive/MyDrive/my_models/efficientnet_model.keras')\n",
        "print(\"EfficientNetB0 model loaded successfully.\")\n",
        "\n",
        "print(\"\\nEvaluation of the loaded EfficientNetB0 model on the test dataset\")\n",
        "loss_efficientnet, accuracy_efficientnet = loaded_model_efficientnet.evaluate(test_dataset)\n",
        "\n",
        "print(f\"\\nTest Loss: {loss_efficientnet:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy_efficientnet:.4f}\")\n",
        "\n",
        "test_predictions_efficientnet = loaded_model_efficientnet.predict(test_dataset, verbose=0)\n",
        "test_predicted_labels_efficientnet = np.argmax(test_predictions_efficientnet, axis=1)\n",
        "\n",
        "test_true_labels = test_dataset.classes\n",
        "\n",
        "class_names = list(train_dataset.class_indices.keys())\n",
        "\n",
        "# Generate Classification Report\n",
        "print(\"\\nClassification Report (EfficientNetB0 Model):\")\n",
        "print(classification_report(test_true_labels, test_predicted_labels_efficientnet, target_names=class_names))\n",
        "\n",
        "# Generate Confusion Matrix\n",
        "print(\"\\nConfusion Matrix (EfficientNetB0 Model):\")\n",
        "cm_efficientnet = confusion_matrix(test_true_labels, test_predicted_labels_efficientnet)\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_efficientnet, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix (EfficientNetB0 Model)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "if 'history_efficientnet' in locals():\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_efficientnet.history['accuracy'])\n",
        "    plt.plot(history_efficientnet.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy (EfficientNetB0)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_efficientnet.history['loss'])\n",
        "    plt.plot(history_efficientnet.history['val_loss'])\n",
        "    plt.title('Model Loss (EfficientNetB0)')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nTraining history object 'history_efficientnet' not found. Cannot plot training history.\")"
      ],
      "metadata": {
        "id": "TRcEg14RYB1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "7ozRCiOzYB1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 6 Hyperparameter Optimization (Transfer Learning with EfficientNetB0)\n",
        "\n",
        "# Load the pretrained model\n",
        "base_model_efficientnet_tune = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "\n",
        "for layer in base_model_efficientnet_tune.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "learning_rates = [0.001, 0.0005]\n",
        "dense_units = [128, 256]\n",
        "dropout_rates = [0.4, 0.5]\n",
        "\n",
        "best_accuracy_efficientnet = 0\n",
        "best_hyperparameters_efficientnet = {}\n",
        "\n",
        "stop_tuning_early_efficientnet = False\n",
        "\n",
        "\n",
        "for lr in learning_rates:\n",
        "    if stop_tuning_early_efficientnet:\n",
        "        break\n",
        "    for units in dense_units:\n",
        "        if stop_tuning_early_efficientnet:\n",
        "            break\n",
        "        for dropout in dropout_rates:\n",
        "            if stop_tuning_early_efficientnet:\n",
        "                break\n",
        "\n",
        "            print(f\"\\n train with LR: {lr}, Dense Units: {units}, Dropout: {dropout}\")\n",
        "\n",
        "\n",
        "            model_tuned_efficientnet = Sequential([\n",
        "                base_model_efficientnet_tune,\n",
        "                layers.GlobalAveragePooling2D(),\n",
        "                layers.Dense(units, activation='relu'),\n",
        "                layers.Dropout(dropout),\n",
        "                layers.Dense(train_dataset.num_classes, activation='softmax')\n",
        "            ])\n",
        "\n",
        "            model_tuned_efficientnet.compile(optimizer=optimizers.Adam(learning_rate=lr),\n",
        "                                         loss='categorical_crossentropy',\n",
        "                                         metrics=['accuracy'])\n",
        "\n",
        "\n",
        "            epochs_tune = 1\n",
        "            history_tuned_efficientnet = model_tuned_efficientnet.fit(\n",
        "                train_dataset,\n",
        "                steps_per_epoch=train_dataset.samples // batch_size,\n",
        "                epochs=epochs_tune,\n",
        "                validation_data=val_dataset,\n",
        "                validation_steps=val_dataset.samples // batch_size,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            val_loss, val_accuracy = model_tuned_efficientnet.evaluate(val_dataset, verbose=0)\n",
        "            print(f\"Validation Accuracy for current trial: {val_accuracy:.4f}\")\n",
        "\n",
        "            if val_accuracy > best_accuracy_efficientnet:\n",
        "                best_accuracy_efficientnet = val_accuracy\n",
        "                best_hyperparameters_efficientnet = {'learning_rate': lr, 'dense_units': units, 'dropout_rate': dropout}\n",
        "                print(\"New best hyperparameters found!\")\n",
        "                stop_tuning_early_efficientnet = True\n",
        "\n",
        "print(\"\\nHyperparameter tuning complete (Simplified Example).\")\n",
        "print(\"Best Validation Accuracy:\", best_accuracy_efficientnet)\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters_efficientnet)"
      ],
      "metadata": {
        "id": "TzSGjj2TYB1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "mbaaCeNqYB1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We employed a simplified manual grid search to explore hyperparameters for the layers added on top of the frozen EfficientNetB0 base (Learning Rate, Dense Units, Dropout Rate). This approach illustrates tuning the classification part of a transfer learning model."
      ],
      "metadata": {
        "id": "LWM3_UENYB1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Y14bJUCYYB1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In the limited tuning trials conducted, a validation accuracy of [Insert Best Validation Accuracy Here] was reached with hyperparameters: [Insert Best Hyperparameters Here]. This indicates the potential for performance changes by optimizing the added layers, subject to a full tuning process."
      ],
      "metadata": {
        "id": "NjRZMyBHYB1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Evaluating the performance of a machine learning model is essential for understanding its effectiveness and its alignment with business objectives. In the context of a multiclass classification model, such as a fish classifier, several key metrics provide varied perspectives on performance:**\n",
        "\n",
        " * **Accuracy:**\n",
        "    * **Implication for Business:** Accuracy reflects the overall correctness of the model, measuring the proportion of correct predictions across all classes.\n",
        "    * **Business Impact:** High accuracy indicates that the model is generally proficient at identifying fish species. In a business context, such as automated sorting or monitoring, high accuracy translates to fewer errors, enhancing operational efficiency and diminishing the necessity for manual verification. However, accuracy can be misleading in cases of imbalanced datasets.\n",
        "\n",
        " * **Precision:**\n",
        "    * **Implication for Business:** Precision assesses the accuracy of positive predictions for a specific class, calculated as the ratio of true positives to the total predicted positives (True Positives + False Positives). It addresses the question: \"Of all instances where the model predicted this fish species, how many were accurate?\"\n",
        "    * **Business Impact:** High precision signifies a lower occurrence of \"false alarms\" or incorrect identifications for that particular species. This metric is particularly significant when the cost of a false positive is substantial, such as mislabeling a protected species, which could result in regulatory complications, or incorrectly categorizing a high-value fish as low-value.\n",
        "\n",
        " * **Recall (Sensitivity):**\n",
        "    * **Implication for Business:** Recall measures the model's capacity to identify all actual positive instances for a specific class, represented by the ratio of true positives to the total actual positives (True Positives + False Negatives). It answers the inquiry: \"Of all actual images of this fish species, how many did the model correctly classify?\"\n",
        "    * **Business Impact:** High recall becomes crucial when the cost of a false negative is elevated, such as failing to detect an invasive species, overlooking a significant fish during a survey, or not identifying a diseased fish. It ensures comprehensive capture of instances within a particular class.\n",
        "\n",
        " * **F1-Score:**\n",
        "    * **Implication for Business:** The F1-score is the harmonic mean of Precision and Recall, providing a single metric that balances both considerations. This measure is particularly advantageous in scenarios with uneven class distribution.\n",
        "    * **Business Impact:** The F1-score presents a holistic view of performance, particularly valuable when both false positives and false negatives carry noteworthy consequences. A high F1-score indicates that the model successfully maintains a balance between accurately identifying positive cases and minimizing the occurrence of false positives.\n",
        "\n",
        " * **Confusion Matrix:**\n",
        "    * **Implication for Business:** A confusion matrix visualizes the performance of a classification model on a designated test dataset, displaying counts of true positives, true negatives, false positives, and false negatives for each class.\n",
        "    * **Business Impact:** This matrix provides an intricate breakdown of the model’s successes and failures. It delineates which classes are often mistaken for others (e.g., misclassifying one fish species as another), thus offering critical insight for identifying specific problem areas that may require additional data, model refinements, or in-depth analyses. This understanding directly influences the reliability and trustworthiness of the model within a business application.\n",
        "\n",
        "- **Overall Business Impact:**\n",
        "\n",
        "  The effective implementation of this machine learning model holds substantial positive implications for business operations:\n",
        "\n",
        " * **Increased Efficiency:** The automation of fish identification significantly diminishes manual labor while expediting processes such as sorting, monitoring, and data collection.\n",
        " * **Improved Accuracy:** A well-functioning model can attain a higher level of consistency and potentially superior accuracy compared to manual methods, thereby reducing errors.\n",
        " * **Cost Reduction:** The decrease in manual effort and a reduction in errors can lead to lower operational expenses.\n",
        " * **Enhanced Monitoring and Compliance:** Accurate identification facilitates improved monitoring of fish populations, aids in the enforcement of fishing regulations, and supports biodiversity tracking.\n",
        " * **Data-Driven Decision Making:** The model supplies valuable insights regarding species composition, which can inform management decisions and contribute to research initiatives.\n"
      ],
      "metadata": {
        "id": "Rn2tR0GM7dCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- While overall **Accuracy** provides a general sense of the model's performance, for a positive business impact in fish classification, metrics that provide a more nuanced view of correct identifications and errors are particularly important. Therefore, we primarily considered **Precision**, **Recall**, and the **F1-score**.\n",
        "\n",
        " *   **Precision** is crucial because a high number of false positives (misidentifying a fish) can lead to significant costs, such as incorrect data collection, mislabeling of products, or even regulatory issues if protected species are wrongly identified. High precision minimizes these costly false alarms.\n",
        " *   **Recall** is equally important because a high number of false negatives (failing to identify a fish that is present) can result in missed opportunities (e.g., failing to count a valuable species), missed detection of problems (e.g., invasive species or disease), or incomplete data for monitoring. High recall ensures that most relevant instances are captured.\n",
        " *   The **F1-score** provides a balanced measure of both Precision and Recall. In most real-world scenarios for fish classification, both false positives and false negatives carry significant consequences. The F1-score helps in finding a model that achieves a good balance between minimizing both types of errors, leading to a more reliable and impactful system.\n",
        "\n",
        "- By focusing on these metrics, we can select a model that not only gets predictions right overall (accuracy) but also specifically manages the types of errors that are most costly or detrimental to the business or conservation goals."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "img_size = (224, 224)\n",
        "\n",
        "model_paths = {\n",
        "    'CNN': '/content/drive/MyDrive/my_models/cnn_model.keras',\n",
        "    'VGG16': '/content/drive/MyDrive/my_models/vgg16_model.keras',\n",
        "    'ResNet50': '/content/drive/MyDrive/my_models/resnet50_model.keras',\n",
        "    'MobileNet': '/content/drive/MyDrive/my_models/mobilenet_model.keras',\n",
        "    'InceptionV3': '/content/drive/MyDrive/my_models/inceptionv3_model.keras',\n",
        "    'EfficientNetB0': '/content/drive/MyDrive/my_models/efficientnet_model.keras'\n",
        "}\n",
        "\n",
        "try:\n",
        "    sample_image_info = test_df.sample(1, random_state=42).iloc[0]\n",
        "    sample_image_path = sample_image_info['filepath']\n",
        "    true_label = sample_image_info['label']\n",
        "    print(f\"Using sample unseen image: {sample_image_path}\")\n",
        "    print(f\"True label: {true_label}\")\n",
        "\n",
        "    img = Image.open(sample_image_path)\n",
        "    img = img.resize(img_size)\n",
        "    img_array = np.array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Display the sample image\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Sample Image (True: {true_label})\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    class_names = list(train_dataset.class_indices.keys())\n",
        "\n",
        "    print(\"\\n--- Predictions for Sample Image ---\")\n",
        "\n",
        "    model_accuracies = {}\n",
        "\n",
        "    for model_name, model_path in model_paths.items():\n",
        "        try:\n",
        "            # Load the model\n",
        "            model = load_model(model_path)\n",
        "\n",
        "            # Make prediction\n",
        "            predictions = model.predict(img_array, verbose=0)\n",
        "            predicted_class_index = np.argmax(predictions)\n",
        "            confidence_score = predictions[0][predicted_class_index]\n",
        "            predicted_label = class_names[predicted_class_index]\n",
        "\n",
        "            print(f\"{model_name} Prediction: {predicted_label} (Confidence: {confidence_score:.4f})\")\n",
        "\n",
        "            if model_name == 'CNN' and 'accuracy_cnn' in locals():\n",
        "                model_accuracies[model_name] = accuracy_cnn\n",
        "            elif model_name == 'VGG16' and 'accuracy_vgg16' in locals():\n",
        "                 model_accuracies[model_name] = accuracy_vgg16\n",
        "            elif model_name == 'ResNet50' and 'accuracy_resnet50' in locals():\n",
        "                 model_accuracies[model_name] = accuracy_resnet50\n",
        "            elif model_name == 'MobileNet' and 'accuracy_mobilenet' in locals():\n",
        "                 model_accuracies[model_name] = accuracy_mobilenet\n",
        "            elif model_name == 'InceptionV3' and 'accuracy_inceptionv3' in locals():\n",
        "                 model_accuracies[model_name] = accuracy_inceptionv3\n",
        "            elif model_name == 'EfficientNetB0' and 'accuracy_efficientnet' in locals():\n",
        "                 model_accuracies[model_name] = accuracy_efficientnet\n",
        "            else:\n",
        "                 model_accuracies[model_name] = \"N/A (Run evaluation cell)\"\n",
        "\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Model file not found for {model_name} at {model_path}\")\n",
        "            model_accuracies[model_name] = \"Error (File Not Found)\"\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during prediction with {model_name}: {e}\")\n",
        "            model_accuracies[model_name] = \"Error (Prediction Failed)\"\n",
        "\n",
        "    print(\"\\n--- Test Accuracy Comparison ---\")\n",
        "    best_model_name = None\n",
        "    highest_accuracy = -1\n",
        "\n",
        "    for name, acc in model_accuracies.items():\n",
        "        if isinstance(acc, (int, float)):\n",
        "            print(f\"{name}: {acc:.4f}\")\n",
        "            if acc > highest_accuracy:\n",
        "                highest_accuracy = acc\n",
        "                best_model_name = name\n",
        "        else:\n",
        "            print(f\"{name}: {acc}\")\n",
        "\n",
        "\n",
        "    if best_model_name and highest_accuracy != -1:\n",
        "        print(f\"\\nBest performing model based on Test Accuracy: {best_model_name} ({highest_accuracy:.4f})\")\n",
        "    else:\n",
        "        print(\"\\nCould not determine the best performing model based on available test accuracies.\")\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The sample image file was not found at {sample_image_path}. Please ensure the file path is correct.\")\n",
        "except NameError as e:\n",
        "    print(f\"Error: Required variables (e.g., test_df, train_dataset) are not defined. Please run the data loading and preprocessing cells.\")\n",
        "    print(f\"Specific error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "1gQZGKm-cfjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Based on the comparative evaluation of all implemented ML models in Section 6, the **MobileNet (ML Model 4)** was chosen as the final prediction model for deployment.\n",
        "\n",
        "- The primary reason for selecting MobileNet was its superior performance on the test dataset, achieving the highest overall accuracy compared to the CNN from scratch, VGG16, ResNet50, InceptionV3, and EfficientNetB0 models under similar training conditions. The evaluation metrics, including the classification report and confusion matrix, further demonstrated MobileNet's effectiveness in correctly classifying most fish species with high confidence.\n",
        "\n",
        "- Additionally, MobileNet is known for being a relatively lightweight and efficient architecture, which is advantageous for deployment scenarios, especially if the application needs to run on devices with limited computational resources. While other models like VGG16 and InceptionV3 also performed well, MobileNet provided the best balance of accuracy and efficiency among the experimented models for this specific task."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7.*** ***Future Work***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "best_model = load_model('/content/drive/MyDrive/my_models/mobilenet_model.keras')\n",
        "print(\"Best model (MobileNet) loaded successfully.\")\n",
        "\n",
        "best_model_save_path = '/content/drive/MyDrive/my_models/best_fish_classifier_model.keras'\n",
        "\n",
        "# Save the best model\n",
        "best_model.save(best_model_save_path)\n",
        "print(f\"\\nBest model saved successfully to: {best_model_save_path}\")"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "best_model_load_path = '/content/drive/MyDrive/my_models/best_fish_classifier_model.keras'\n",
        "\n",
        "loaded_best_model = load_model(best_model_load_path)\n",
        "print(f\"Best model loaded successfully from: {best_model_load_path}\")\n",
        "\n",
        "sample_image_info = test_df.sample(1, random_state=42).iloc[0]\n",
        "sample_image_path = sample_image_info['filepath']\n",
        "true_label = sample_image_info['label']\n",
        "print(f\"\\nLoading sample unseen image: {sample_image_path}\")\n",
        "print(f\"True label: {true_label}\")\n",
        "\n",
        "# Load and preprocess the sample image\n",
        "try:\n",
        "    img = Image.open(sample_image_path)\n",
        "    img = img.resize(img_size)\n",
        "    img_array = np.array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Make a prediction\n",
        "    predictions = loaded_best_model.predict(img_array)\n",
        "\n",
        "    predicted_class_index = np.argmax(predictions)\n",
        "    confidence_score = predictions[0][predicted_class_index]\n",
        "\n",
        "    class_names = list(train_dataset.class_indices.keys())\n",
        "    predicted_label = class_names[predicted_class_index]\n",
        "\n",
        "    print(f\"\\nPredicted label: {predicted_label}\")\n",
        "    print(f\"Confidence score: {confidence_score:.4f}\")\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"True: {true_label}\\nPredicted: {predicted_label} ({confidence_score:.2f})\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The sample image file was not found at {sample_image_path}. Please ensure the file path is correct.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during prediction: {e}\")"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa99ad13"
      },
      "source": [
        "## Streamlit Deployment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "8e169c02"
      },
      "source": [
        "# Install dependencies\n",
        "!pip install streamlit ngrok pyngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5bce119"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Define image size expected by the model\n",
        "img_size = (224, 224)\n",
        "\n",
        "# Define the path to the saved best model\n",
        "best_model_load_path = '/content/drive/MyDrive/my_models/best_fish_classifier_model.keras'\n",
        "\n",
        "class_indices_path = '/content/drive/MyDrive/my_data/class_indices.json'\n",
        "\n",
        "# Load the saved model\n",
        "@st.cache_resource\n",
        "def load_model(model_path):\n",
        "    \"\"\"Loads the pre-trained Keras model.\"\"\"\n",
        "    try:\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading model: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load class indices mapping\n",
        "@st.cache_resource\n",
        "def load_class_indices(indices_path):\n",
        "    \"\"\"Loads the class indices mapping from a JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(indices_path, 'r') as f:\n",
        "            class_indices = json.load(f)\n",
        "        # Invert the dictionary to get index -> class name mapping\n",
        "        return {v: k for k, v in class_indices.items()}\n",
        "    except FileNotFoundError:\n",
        "        st.error(f\"Class indices file not found at {indices_indices_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading class indices: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "model = load_model(best_model_load_path)\n",
        "idx_to_class = load_class_indices(class_indices_path)\n",
        "\n",
        "# Streamlit application layout\n",
        "st.title(\"Multiclass Fish Image Classification\")\n",
        "\n",
        "st.write(\"Upload an image of a fish to classify it.\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    try:\n",
        "        # Display the uploaded image\n",
        "        image = Image.open(uploaded_file)\n",
        "        st.image(image, caption=\"Uploaded Image.\", use_column_width=True)\n",
        "\n",
        "        # Preprocess the image for prediction\n",
        "        img = image.resize(img_size)\n",
        "        img_array = np.array(img) / 255.0\n",
        "        img_array = np.expand_dims(img_array, axis=0) # Add batch dimension\n",
        "\n",
        "        if model is not None and idx_to_class is not None:\n",
        "            # Make prediction\n",
        "            st.write(\"Classifying...\")\n",
        "            predictions = model.predict(img_array)\n",
        "            predicted_class_index = np.argmax(predictions)\n",
        "            confidence_score = predictions[0][predicted_class_index]\n",
        "\n",
        "            # Get predicted label\n",
        "            predicted_label = idx_to_class.get(predicted_class_index, \"Unknown\")\n",
        "\n",
        "            # Display results\n",
        "            st.write(f\"Prediction: **{predicted_label}**\")\n",
        "            st.write(f\"Confidence: **{confidence_score:.2f}**\")\n",
        "        elif model is None:\n",
        "            st.error(\"Model could not be loaded. Please check the model path.\")\n",
        "        else: # idx_to_class is None\n",
        "             st.error(\"Class indices could not be loaded. Cannot interpret prediction results.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred during image processing or prediction: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bd645ef"
      },
      "source": [
        "# Set up ngrok tunnel by querying the local API\n",
        "\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "\n",
        "auth_token = \"2zfEFhgAORGwzbHpkesKDMBDyx5_nmnrB3QYJTr46jy9HhZ8\"\n",
        "try:\n",
        "    ngrok_config_path = f\"{os.path.expanduser('~')}/.ngrok2/ngrok.yml\"\n",
        "    config_content = \"\"\n",
        "    if os.path.exists(ngrok_config_path):\n",
        "        with open(ngrok_config_path, 'r') as f:\n",
        "            config_content = f.read()\n",
        "\n",
        "    if auth_token not in config_content:\n",
        "        print(\"Authenticating ngrok...\")\n",
        "        !ngrok authtoken {auth_token}\n",
        "        print(\"ngrok authentication complete.\")\n",
        "    else:\n",
        "        print(\"ngrok authentication token already set.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during ngrok authentication setup: {e}\")\n",
        "\n",
        "try:\n",
        "    print(\"\\nStarting ngrok tunnel to port 8501 in the background...\")\n",
        "    !pkill ngrok > /dev/null 2>&1\n",
        "    get_ipython().system_raw('ngrok http 8501 &')\n",
        "    print(\"ngrok agent started in the background.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error starting ngrok agent: {e}\")\n",
        "\n",
        "\n",
        "print(\"Waiting for ngrok tunnel to establish...\")\n",
        "time.sleep(5)\n",
        "\n",
        "\n",
        "ngrok_api_url = \"http://localhost:4040/api/tunnels\"\n",
        "public_url = None\n",
        "\n",
        "try:\n",
        "    response = requests.get(ngrok_api_url)\n",
        "    response.raise_for_status()\n",
        "    tunnel_data = response.json()\n",
        "\n",
        "\n",
        "    for tunnel in tunnel_data['tunnels']:\n",
        "        if tunnel['proto'] == 'http':\n",
        "            public_url = tunnel['public_url']\n",
        "            break\n",
        "\n",
        "    if public_url:\n",
        "        print(f\"\\nNgrok public URL: {public_url}\")\n",
        "    else:\n",
        "        print(\"\\nError: Could not find the ngrok public URL from the API.\")\n",
        "        print(\"Tunnel data:\", tunnel_data)\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"\\nError querying ngrok API: {e}\")\n",
        "    print(\"Please check if the ngrok agent started correctly and is running.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn unexpected error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run streamlit app\n",
        "!streamlit run app.py"
      ],
      "metadata": {
        "id": "Rdo9W_apsy6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This project successfully delivered a comprehensive end-to-end deep learning pipeline for multiclass fish image classification. The process began with the extraction and preprocessing of the raw dataset, progressed through exploratory data analysis (EDA), model building, and evaluation, and culminated in deployment within a Streamlit web application.\n",
        "\n",
        " **Key Achievements Include:**\n",
        "\n",
        "  1. **Data Preparation & Quality Assurance:**\n",
        "   The dataset underwent rigorous validation to ensure the absence of missing, duplicate, or corrupt images. Standardization to a consistent input size was implemented, and data augmentation techniques were utilized to enhance the model's robustness against variations in pose, lighting, and background.\n",
        "\n",
        "  2. **Comprehensive EDA:**\n",
        "   A series of over a dozen visualizations were generated to provide insights into class distribution, image properties, and feature characteristics, such as brightness and color variations. This thorough EDA facilitated informed modeling decisions and helped clarify potential challenges and opportunities within the dataset.\n",
        "\n",
        "  3. **Model Experimentation:**\n",
        "   A custom Convolutional Neural Network (CNN) was architected from scratch, alongside the implementation of five transfer learning architectures: VGG16, ResNet50, MobileNet, InceptionV3, and EfficientNetB0. Fine-tuning strategies were applied, and model training was optimized through the use of callbacks, including early stopping and learning rate reduction.\n",
        "\n",
        "  4. **Evaluation & Selection:**\n",
        "   A comprehensive evaluation of all models was conducted using various metrics: accuracy, precision, recall, F1-score, and confusion matrices. Through this analysis, the best-performing model was identified, showcasing high accuracy and balanced class performance, and was subsequently saved for deployment.\n",
        "\n",
        "  5. **Explainability & Deployment:**\n",
        "   The project integrated Grad-CAM for visual model interpretation, allowing for insights into the model's decision-making process. The chosen model was deployed as a Streamlit web application, providing real-time user predictions accompanied by confidence scores.\n",
        "\n",
        "- **Final Outcome:**\n",
        "The solution demonstrates that transfer learning, when paired with meticulously tailored fine-tuning, can substantially outperform a CNN developed from scratch in the context of visual classification tasks. The deployed application serves as an intuitive and accessible tool for instant fish species recognition, offering valuable applications in fisheries management, marine research, and the seafood industry.\n",
        "\n",
        "- **Future Improvements Could Include:**\n",
        "   - **Expanding the Dataset:** Enhancing the dataset by incorporating more species and diverse image conditions could improve model generalization.\n",
        "   - **Advanced Hyperparameter Tuning and Ensemble Learning:** Implementing more sophisticated hyperparameter tuning and experimenting with ensemble learning techniques could further enhance model performance.\n",
        "   - **Domain-Specific Augmentation:** Exploring domain-specific augmentation techniques, such as underwater blur simulation, may improve robustness in real-world scenarios.\n",
        "   - **Cloud Deployment:** Transitioning the model to a production cloud environment could facilitate broader access and scalability.\n",
        "\n",
        "- Overall, this project exemplifies the full machine learning lifecycle—data acquisition, model development, evaluation, and deployment—effectively showcasing a practical, accurate, and interpretable deep learning solution primed for real-world application."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}